
@article{Breiman2001,
	title        = {Random Forests},
	author       = {Breiman, Leo},
	year         = 2001,
	month        = {Oct},
	day          = {01},
	journal      = {Machine Learning},
	volume       = 45,
	number       = 1,
	pages        = {5--32},
	doi          = {10.1023/A:1010933404324},
	doi          = {10.1023/A:1010933404324},
	issn         = {1573-0565},
	abstract     = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.}
}
@inproceedings{Shahhosseini2001,
	title        = {Optimizing Ensemble Weights for Machine Learning Models: A Case Study for Housing Price Prediction},
	author       = {Shahhosseini, Mohsen and Hu, Guiping and Pham, Hieu},
	year         = 2020,
	booktitle    = {Smart Service Systems, Operations Management, and Analytics},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {87--97},
	isbn         = {978-3-030-30967-1},
	editor       = {Yang, Hui and Qiu, Robin and Chen, Weiwei},
	abstract     = {Designing ensemble learners has been recognized as one of the significant trends in the field of data knowledge, especially, in data science competitions. Building models that are able to outperform all individual models in terms of bias, which is the error due to the difference in the average model predictions and actual values, and variance, which is the variability of model predictions, has been the main goal of the studies in this area. An optimization model has been proposed in this paper to design ensembles that try to minimize bias and variance of predictions. Focusing on service sciences, two well-known housing datasets have been selected as case studies: Boston housing and Ames housing. The results demonstrate that our designed ensembles can be very competitive in predicting the house prices in both Boston and Ames datasets.}
}
@inproceedings{neloy2019,
	title        = {Ensemble Learning Based Rental Apartment Price Prediction Model by Categorical Features Factoring},
	author       = {Neloy, Asif Ahmed and Haque, H. M. Sadman and Ul Islam, Md. Mahmud},
	year         = 2019,
	booktitle    = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing - ICMLC '19},
	publisher    = {ACM Press},
	address      = {Zhuhai, China},
	pages        = {350--356},
	doi          = {10.1145/3318299.3318377}
}
@article{AstiviaZumbo2019,
	title        = {Heteroskedasticity in Multiple Regression Analysis: What it is, How to Detect it and How to Solve it with Applications in R and SPSS},
	author       = {Astivia, Oscar L. Olvera and Zumbo, Bruno D.},
	year         = 2019,
	journal      = {Practical Assessment, Research, and Evaluation},
	volume       = 24,
	number       = 1,
	doi          = {10.7275/q5xr-fr95},
	url          = {https://scholarworks.umass.edu/pare/vol24/iss1/1}
}
@article{Schmidhuber2015,
	title        = {Deep Learning in Neural Networks: An Overview},
	author       = {Schmidhuber, Juergen},
	year         = 2015,
	journal      = {Neural Networks},
	volume       = 61,
	pages        = {85--117},
	doi          = {10.1016/j.neunet.2014.09.003},
	url          = {https://arxiv.org/abs/1404.7828}
}
@misc{scikit-learn,
	title        = {Scikit-learn: Machine Learning in {Python}},
	author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year         = 2011,
	journal      = {Journal of Machine Learning Research},
	volume       = 12,
	pages        = {2825--2830}
}
@misc{kingma2017adam,
	title        = {Adam: A Method for Stochastic Optimization},
	author       = {Diederik P. Kingma and Jimmy Ba},
	year         = 2017,
	doi          = {10.48550/arXiv.1412.6980},
	eprint       = {1412.6980},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{agarap2019deep,
	title        = {Deep Learning using Rectified Linear Units (ReLU)},
	author       = {Abien Fred Agarap},
	year         = 2019,
	doi          = {10.48550/arXiv.1803.08375},
	eprint       = {1803.08375},
	archiveprefix = {arXiv},
	primaryclass = {cs.NE}
}
