@inproceedings{neloy2019,
	title = {Ensemble Learning Based Rental Apartment Price Prediction Model by
	         Categorical Features Factoring},
	author = {Neloy, Asif Ahmed and Haque, H. M. Sadman and Ul Islam, Md. Mahmud },
	year = 2019,
	booktitle = {Proceedings of the 2019 11th International Conference on Machine
	             Learning and Computing - ICMLC '19},
	publisher = {ACM Press},
	address = {Zhuhai, China},
	pages = {350--356},
	doi = {10.1145/3318299.3318377},
}

@article{Aoki2023,
	author = {Aoki, Yoshitaka and Suzuki, Yuji and Nakajima, Yoshiki},
	title = {Critical considerations, including overfitting in regression models
	         and confounding in study designs for delirium follow-up},
	journal = {Journal of Anesthesia},
	year = 2023,
	month = 4,
	day = 1,
	volume = 37,
	number = 2,
	pages = {321-322},
	issn = {1438-8359},
	doi = {10.1007/s00540-022-03157-1},
	url = {https://doi.org/10.1007/s00540-022-03157-1},
}

@article{Stekhoven2011,
	author = {Stekhoven, Daniel J. and Bühlmann, Peter},
	title = "{MissForest—non-parametric missing value imputation for mixed-type
	         data}",
	journal = {Bioinformatics},
	volume = 28,
	number = 1,
	pages = {112--118},
	year = 2011,
	month = 10,
	abstract = { Modern data acquisition based on high-throughput technology is
	            often facing the problem of missing data. Algorithms commonly used
	            in the analysis of such large-scale data often depend on a complete
	            set. Missing value imputation offers a solution to this problem.
	            However, the majority of available imputation methods are
	            restricted to one type of variable only: continuous or categorical.
	            For mixed-type data, the different types are usually handled
	            separately. Therefore, these methods ignore possible relations
	            between variable types. We propose a non-parametric method which
	            can cope with different types of variables simultaneously.Results:
	            We compare several state of the art methods for the imputation of
	            missing values. We propose and evaluate an iterative imputation
	            method (missForest) based on a random forest. By averaging over
	            many unpruned classification or regression trees, random forest
	            intrinsically constitutes a multiple imputation scheme. Using the
	            built-in out-of-bag error estimates of random forest, we are able
	            to estimate the imputation error without the need of a test set.
	            Evaluation is performed on multiple datasets coming from a diverse
	            selection of biological fields with artificially introduced missing
	            values ranging from 10 to 30 percent. We show that missForest can
	            successfully handle missing values, particularly in datasets
	            including different types of variables. In our comparative study,
	            missForest outperforms other methods of imputation especially in
	            data settings where complex interactions and non-linear relations
	            are suspected. The out-of-bag imputation error estimates of
	            missForest prove to be adequate in all settings. Additionally,
	            missForest exhibits attractive computational efficiency and can
	            cope with high-dimensional data. },
	issn = {1367-4803},
	doi = {10.1093/bioinformatics/btr597},
	url = {https://doi.org/10.1093/bioinformatics/btr597},
	eprint = {
	          https://academic.oup.com/bioinformatics/article-pdf/28/1/112/50568519/bioinformatics
	          \_28\_1\_112.pdf},
}

@techreport{Crudu2022,
	title = {On the Role of the Zero Conditional Mean Assumption for Causal
	         Inference in Linear Models},
	author = {Crudu, Federico and Knaus, Michael and Mellace, Giovanni and Smits ,
	          Joeri},
	year = 2022,
	institution = {arXiv.org},
	type = {Papers},
	abstract = {Many econometrics textbooks imply that under mean independence of
	            the regressors and the error term, the OLS parameters have a causal
	            interpretation. We show that even when this assumption is satisfied
	            , OLS might identify a pseudo-parameter that does not have a causal
	            interpretation. Even assuming that the linear model is "structural"
	            creates some ambiguity in what the regression error represents and
	            whether the OLS estimand is causal. This issue applies equally to
	            linear IV and panel data models. To give these estimands a causal
	            interpretation, one needs to impose assumptions on a "causal" model
	            , e.g., using the potential outcome framework. This highlights that
	            causal inference requires causal, and not just stochastic,
	            assumptions.},
	url = {https://EconPapers.repec.org/RePEc:arx:papers:2211.09502},
}

@article{Yang2019,
	author = {Kun Yang and Justin Tu and Tian Chen},
	title = {Homoscedasticity: an overlooked critical assumption for linear
	         regression},
	volume = 32,
	number = 5,
	year = 2019,
	doi = {10.1136/gpsych-2019-100148},
	publisher = {General Psychiatry},
	abstract = {Linear regression is widely used in biomedical and psychosocial
	            research. A critical assumption that is often overlooked is
	            homoscedasticity. Unlike normality, the other assumption on data
	            distribution, homoscedasticity is often taken for granted when
	            fitting linear regression models. However, contrary to popular
	            belief, this assumption actually has a bigger impact on validity of
	            linear regression results than normality. In this report, we use
	            Monte Carlo simulation studies to investigate and compare their
	            effects on validity of inference.},
	URL = {https://gpsych.bmj.com/content/32/5/e100148},
	eprint = {https://gpsych.bmj.com/content/32/5/e100148.full.pdf},
	journal = {General Psychiatry},
}


@inbook{Baltagi2011,
	author = "Baltagi, Badi H.",
	title = "Violations of the Classical Assumptions",
	bookTitle = "Econometrics",
	year = 2011,
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "95--129",
	abstract = "In this chapter, we relax the assumptions made in Chapter 3 one by
	            one and study the effect of that on the OLS estimator. In case the
	            OLS estimator is no longer a viable estimator, we derive an
	            alternative estimator and propose some tests that will allow us to
	            check whether this assumption is violated.",
	isbn = "978-3-642-20059-5",
	doi = "10.1007/978-3-642-20059-5_5",
	url = "https://doi.org/10.1007/978-3-642-20059-5_5",
}

@article{schmidt2018,
	title = {Linear regression and the normality assumption},
	author = {Schmidt, Amand F and Finan, Chris},
	journal = {Journal of Clinical Epidemiology},
	volume = 98,
	pages = {146--151},
	year = 2018,
	publisher = {Elsevier},
}

@article{shrestha2020,
	title = {Detecting Multicollinearity in Regression Analysis},
	author = {Shrestha, Noora},
	journal = {American Journal of Applied Mathematics and Statistics},
	volume = 8,
	number = 2,
	pages = {39--42},
	year = 2020,
	doi = {10.12691/ajams-8-2-1},
}

@inbook{cutler2011,
	author = {Cutler, Adele and Cutler, David and Stevens, John},
	year = 2011,
	month = 1,
	pages = "157-176",
	title = {Random Forests},
	volume = 45,
	isbn = {978-1-4419-9325-0},
	journal = {Machine Learning - ML},
	doi = {10.1007/978-1-4419-9326-7_5},
}

@misc{zou2021,
	title = {Understanding the Generalization of Adam in Learning Neural Networks
	         with Proper Regularization},
	author = {Difan Zou and Yuan Cao and Yuanzhi Li and Quanquan Gu},
	year = 2021,
	eprint = {2108.11371},
	archivePrefix = {arXiv},
	primaryClass = {cs.LG},
}

@misc{Steorts15,
	author = {Steorts, Rebecca C.},
	title = {Bagging and Random Forests},
	year = 2015,
	url = {https://www2.stat.duke.edu/~
	       rcs46/lectures_2015/random-forest/randomforests.pdf},
}

@misc{jin2022,
	title = {Hyperparameter Importance for Machine Learning Algorithms},
	author = {Honghe Jin},
	year = 2022,
	eprint = {2201.05132},
	archivePrefix = {arXiv},
	primaryClass = {stat.ML},
}

@book{pytlak2009,
	title = {Conjugate Gradient Algorithms in Nonconvex Optimization},
	author = {Radosław Pytlak},
	publisher = {Springer},
	isbn = {9783540856337},
	year = 2009,
	series = {Nonconvex Optimization and Its Applications №89},
	edition = 1,
	url = {libgen.li/file.php?md5=0488762b1858b9f00c7d894c34c59938},
}

@book{sra2011,
	title = {Optimization for Machine Learning (Neural Information Processing
	         series) },
	author = {Suvrit Sra, Sebastian Nowozin, Stephen J. Wright},
	publisher = {The MIT Press},
	isbn = {9780262016469},
	year = 2011,
	series = {Neural Information Processing series},
	url = {libgen.li/file.php?md5=7d0be65e811df10dd025655b433091ee},
}

@article{pek2018,
	AUTHOR = {Pek, Jolynn and Wong, Octavia and Wong, Augustine C. M.},
	TITLE = {How to Address Non-normality: A Taxonomy of Approaches, Reviewed, and
	         Illustrated},
	JOURNAL = {Frontiers in Psychology},
	VOLUME = 9,
	YEAR = 2018,
	URL = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.02104},
	DOI = {10.3389/fpsyg.2018.02104},
	ISSN = {1664-1078},
	ABSTRACT = {The linear model often serves as a starting point for applying
	            statistics in psychology. Often, formal training beyond the linear
	            model is limited, creating a potential pedagogical gap because of
	            the pervasiveness of data non-normality. We reviewed 61 recently
	            published undergraduate and graduate textbooks on introductory
	            statistics and the linear model, focusing on their treatment of
	            non-normality. This review identified at least eight distinct
	            methods suggested to address non-normality, which we organize into
	            a new taxonomy according to whether the approach: (a) remains
	            within the linear model, (b) changes the data, and (c) treats
	            normality as informative or as a nuisance. Because textbook
	            coverage of these methods was often cursory, and methodological
	            papers introducing these approaches are usually inaccessible to
	            non-statisticians, this review is designed to be the happy medium.
	            We provide a relatively non-technical review of advanced methods
	            which can address non-normality (and heteroscedasticity), thereby
	            serving a starting point to promote best practice in the
	            application of the linear model. We also present three empirical
	            examples to highlight distinctions between these methods'
	            motivations and results. The paper also reviews the current state
	            of methodological research in addressing non-normality within the
	            linear modeling framework. It is anticipated that our taxonomy will
	            provide a useful overview and starting place for researchers
	            interested in extending their knowledge in approaches developed to
	            address non-normality from the perspective of the linear model.},
}

@article{walsch2021,
	title = {Exploring the effects of omitted variable bias in physics education
	         research},
	author = {Walsh, Cole and Stein, Martin M. and Tapping, Ryan and Smith, Emily
	          M. and Holmes, N. G.},
	journal = {Phys. Rev. Phys. Educ. Res.},
	volume = 17,
	issue = 1,
	pages = {010119},
	numpages = 8,
	year = 2021,
	month = 3,
	publisher = {American Physical Society},
	doi = {10.1103/PhysRevPhysEducRes.17.010119},
	url = {https://link.aps.org/doi/10.1103/PhysRevPhysEducRes.17.010119},
}

@misc{kingma2017,
	title = {Adam: A Method for Stochastic Optimization},
	author = {Diederik P. Kingma and Jimmy Ba},
	year = 2017,
	eprint = {1412.6980},
	archivePrefix = {arXiv},
	primaryClass = {cs.LG},
}

@article{marcot2021,
	author = {Marcot, Bruce G. and Hanea, Anca M.},
	title = {What is an optimal value of k in k-fold cross-validation in discrete
	         Bayesian network analysis?},
	journal = {Computational Statistics},
	year = 2021,
	month = 9,
	day = 1,
	volume = 36,
	number = 3,
	pages = {2009--2031},
	abstract = {Cross-validation using randomized subsets of data---known as
	            k-fold cross-validation---is a powerful means of testing the
	            success rate of models used for classification. However, few if any
	            studies have explored how values of k (number of subsets) affect
	            validation results in models tested with data of known statistical
	            properties. Here, we explore conditions of sample size, model
	            structure, and variable dependence affecting validation outcomes in
	            discrete Bayesian networks (BNs). We created 6 variants of a BN
	            model with known properties of variance and collinearity, along
	            with data sets of n{\thinspace}={\thinspace}50, 500, and 5000
	            samples, and then tested classification success and evaluated CPU
	            computation time with seven levels of folds (k{\thinspace}={
	            \thinspace}2, 5, 10, 20, n − 5, n − 2, and n − 1). Classification
	            error declined with increasing n, particularly in BN models with
	            high multivariate dependence, and declined with increasing k,
	            generally levelling out at k{\thinspace}={\thinspace}10, although k
	            {\thinspace}={\thinspace}5 sufficed with large samples (n{
	            \thinspace}={\thinspace}5000). Our work supports the common use of
	            k{\thinspace}={\thinspace}10 in the literature, although in some
	            cases k{\thinspace}={\thinspace}5 would suffice with BN models
	            having independent variable structures.},
	issn = {1613-9658},
	doi = {10.1007/s00180-020-00999-9},
	url = {https://doi.org/10.1007/s00180-020-00999-9},
}

@article{rosen1974,
	author = {Rosen, Sherwin},
	title = {Hedonic Prices and Implicit Markets: Product Differentiation in Pure
	         Competition},
	journal = {Journal of Political Economy},
	volume = 82,
	number = 1,
	pages = {34--55},
	year = 1974,
	doi = {10.1086/260169},
	url = {https://doi.org/10.1086/260169},
}


@inbook{taylor2003,
	author = "Taylor, Laura O.",
	editor = "Champ, Patricia A. and Boyle, Kevin J. and Brown, Thomas C.",
	title = "The Hedonic Method",
	bookTitle = "A Primer on Nonmarket Valuation",
	year = 2003,
	publisher = "Springer Netherlands",
	address = "Dordrecht",
	pages = "331--393",
	abstract = "Heterogeneous or differentiated goods are products whose
	            characteristics vary in such a way that there are distinct product
	            varieties even though the commodity is sold in one market (e.g.,
	            cars, computers, houses). The variation in product variety gives
	            rise to variations in product prices within each market. The
	            hedonic method for non-market valuation relies on market
	            transactions for these differentiated goods to determine the value
	            of key underlying characteristics. For instance, by observing the
	            price differential between two product varieties that vary only by
	            one characteristic (e.g., two identical cars, but with one having
	            more horsepower than the other), we indirectly observe the monetary
	            trade-offs individuals are willing to make with respect to the
	            changes in this characteristic. As such, the hedonic method is an
	            ``indirect'' valuation method in which we do not observe the value
	            consumers have for the characteristic directly, but infer it from
	            observable market transactions.",
	isbn = "978-94-007-0826-6",
	doi = "10.1007/978-94-007-0826-6_10",
	url = "https://doi.org/10.1007/978-94-007-0826-6_10",
}

@inproceedings{joshi2022,
	author = "Joshi, Ishan and Mudgil, Pooja and Bisht, Arpit",
	editor = "Gupta, Deepak and Khanna, Ashish and Hassanien, Aboul Ella and Anand
	          , Sameer and Jaiswal, Ajay",
	title = "House Price Forecasting by Implementing Machine Learning Algorithms:
	         A Comparative Study",
	booktitle = "International Conference on Innovative Computing and
	             Communications",
	year = 2022,
	publisher = "Springer Nature Singapore",
	address = "Singapore",
	pages = "63--71",
	abstract = "Discerning property value via state-of-the-art machine learning
	            techniques can evolve the current real-estate market and expose it
	            to the technological frontiers of the modern world. This can
	            potentially have sanguine domino effects such as opening the market
	            to new investors as a result of technically backed price values.
	            The current research paper strives to capitalize on this
	            opportunity by analyzing information and data from an existing
	            online marketplace for buyers and sellers in this industry. It is
	            conjectured that precise prediction of house prices in a particular
	            location through data analytics will create a candid market where
	            prices are not arbitrary, ensuring openness in the market through
	            P2P opportunities which will eliminate middleman charges. The
	            research ventures to extrapolate machine learning techniques to
	            create a model that predicts house prices in Bangalore using a
	            plethora of algorithms such as linear regression, bagging
	            classifier, K-nearest neighbour, XGB, decision tree, gradient
	            boosting, and random forest. An incremental approach is deployed to
	            gather and streamline data, clean, visualize, model and evaluate
	            the models produced. The research is completed with a result from
	            the comparative study, showing the most appropriate algorithm for
	            the given data available is the random forest algorithm.",
	isbn = "978-981-19-3679-1",
}

@article{zietz2008,
	author = {Zietz, Joachim and Zietz, Emily Norman and Sirmans, G. Stacy},
	title = {Determinants of House Prices: A Quantile Regression Approach},
	journal = {The Journal of Real Estate Finance and Economics},
	year = 2008,
	month = 11,
	day = 1,
	volume = 37,
	number = 4,
	pages = "317-333",
	abstract = {OLS regression has typically been used in housing research to
	            determine the relationship of a particular housing characteristic
	            with selling price. Results differ across studies, not only in
	            terms of size of OLS coefficients and statistical significance, but
	            sometimes in direction of effect. This study suggests that some of
	            the observed variation in the estimated prices of housing
	            characteristics may reflect the fact that characteristics are not
	            priced the same across a given distribution of house prices. To
	            examine this issue, this study uses quantile regression, with and
	            without accounting for spatial autocorrecation, to identify the
	            coefficients of a large set of diverse variables across different
	            quantiles. The results show that purchasers of higher-priced homes
	            value certain housing characteristics such as square footage and
	            the number of bathrooms differently from buyers of lower-priced
	            homes. Other variables such as age are also shown to vary across
	            the distribution of house prices.},
	issn = {1573-045X},
	doi = {10.1007/s11146-007-9053-7},
	url = {https://doi.org/10.1007/s11146-007-9053-7},
}

@article{yoshida2022,
	author = {Yoshida, Takahiro and Murakami, Daisuke and Seya, Hajime},
	title = {Spatial Prediction of Apartment Rent using Regression-Based and
	         Machine Learning-Based Approaches with a Large Dataset},
	journal = {The Journal of Real Estate Finance and Economics},
	year = 2022,
	month = 11,
	day = 28,
	abstract = {Employing a large dataset (at most, the order of n{\thinspace}={
	            \thinspace}106), this study attempts enhance the literature on the
	            comparison between regression and machine learning-based rent price
	            prediction models by adding new empirical evidence and considering
	            the spatial dependence of the observations. The regression-based
	            approach incorporates the nearest neighbor Gaussian processes
	            (NNGP) model, enabling the application of kriging to large
	            datasets. In contrast, the machine learning-based approach utilizes
	            typical models: extreme gradient boosting (XGBoost), random forest
	            (RF), and deep neural network (DNN). The out-of-sample prediction
	            accuracy of these models was compared using Japanese apartment rent
	            data, with a varying order of sample sizes (i.e., n{\thinspace}={
	            \thinspace}104, 105, 106). The results showed that, as the sample
	            size increased, XGBoost and RF outperformed NNGP with higher
	            out-of-sample prediction accuracy. XGBoost achieved the highest
	            prediction accuracy for all sample sizes and error measures in both
	            logarithmic and real scales and for all price bands if the
	            distribution of rents is similar in training and test data. A
	            comparison of several methods to account for the spatial dependence
	            in RF showed that simply adding spatial coordinates to the
	            explanatory variables may be sufficient.},
	issn = {1573-045X},
	doi = {10.1007/s11146-022-09929-6},
	url = {https://doi.org/10.1007/s11146-022-09929-6},
}
