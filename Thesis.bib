@inproceedings{neloy2019,
	title = {Ensemble Learning Based Rental Apartment Price Prediction Model by
	         Categorical Features Factoring},
	author = {Neloy, Asif Ahmed and Haque, H. M. Sadman and Ul Islam, Md. Mahmud },
	year = 2019,
	booktitle = {Proceedings of the 2019 11th International Conference on Machine
	             Learning and Computing - ICMLC '19},
	publisher = {ACM Press},
	address = {Zhuhai, China},
	pages = {350--356},
	doi = {10.1145/3318299.3318377},
}

@inproceedings{Shahhosseini2001,
	title = {Optimizing Ensemble Weights for Machine Learning Models: A Case Study
	         for Housing Price Prediction},
	author = {Shahhosseini, Mohsen and Hu, Guiping and Pham, Hieu},
	year = 2020,
	booktitle = {Smart Service Systems, Operations Management, and Analytics},
	publisher = {Springer International Publishing},
	address = {Cham},
	pages = {87--97},
	isbn = {978-3-030-30967-1},
	editor = {Yang, Hui and Qiu, Robin and Chen, Weiwei},
	abstract = {Designing ensemble learners has been recognized as one of the
	            significant trends in the field of data knowledge, especially, in
	            data science competitions. Building models that are able to
	            outperform all individual models in terms of bias, which is the
	            error due to the difference in the average model predictions and
	            actual values, and variance, which is the variability of model
	            predictions, has been the main goal of the studies in this area. An
	            optimization model has been proposed in this paper to design
	            ensembles that try to minimize bias and variance of predictions.
	            Focusing on service sciences, two well-known housing datasets have
	            been selected as case studies: Boston housing and Ames housing. The
	            results demonstrate that our designed ensembles can be very
	            competitive in predicting the house prices in both Boston and Ames
	            datasets.},
}


@article{Aoki2023,
	author = {Aoki, Yoshitaka and Suzuki, Yuji and Nakajima, Yoshiki},
	title = {Critical considerations, including overfitting in regression models
	         and confounding in study designs for delirium follow-up},
	journal = {Journal of Anesthesia},
	year = 2023,
	month = {Apr},
	day = {01},
	volume = 37,
	number = 2,
	pages = {321-322},
	issn = {1438-8359},
	doi = {10.1007/s00540-022-03157-1},
	url = {https://doi.org/10.1007/s00540-022-03157-1},
}

@article{Stekhoven2011,
	author = {Stekhoven, Daniel J. and Bühlmann, Peter},
	title = "{MissForest—non-parametric missing value imputation for mixed-type
	         data}",
	journal = {Bioinformatics},
	volume = 28,
	number = 1,
	pages = {112--118},
	year = 2011,
	month = 10,
	abstract = { Modern data acquisition based on high-throughput technology is
	            often facing the problem of missing data. Algorithms commonly used
	            in the analysis of such large-scale data often depend on a complete
	            set. Missing value imputation offers a solution to this problem.
	            However, the majority of available imputation methods are
	            restricted to one type of variable only: continuous or categorical.
	            For mixed-type data, the different types are usually handled
	            separately. Therefore, these methods ignore possible relations
	            between variable types. We propose a non-parametric method which
	            can cope with different types of variables simultaneously.Results:
	            We compare several state of the art methods for the imputation of
	            missing values. We propose and evaluate an iterative imputation
	            method (missForest) based on a random forest. By averaging over
	            many unpruned classification or regression trees, random forest
	            intrinsically constitutes a multiple imputation scheme. Using the
	            built-in out-of-bag error estimates of random forest, we are able
	            to estimate the imputation error without the need of a test set.
	            Evaluation is performed on multiple datasets coming from a diverse
	            selection of biological fields with artificially introduced missing
	            values ranging from 10 to 30 percent. We show that missForest can
	            successfully handle missing values, particularly in datasets
	            including different types of variables. In our comparative study,
	            missForest outperforms other methods of imputation especially in
	            data settings where complex interactions and non-linear relations
	            are suspected. The out-of-bag imputation error estimates of
	            missForest prove to be adequate in all settings. Additionally,
	            missForest exhibits attractive computational efficiency and can
	            cope with high-dimensional data. },
	issn = {1367-4803},
	doi = {10.1093/bioinformatics/btr597},
	url = {https://doi.org/10.1093/bioinformatics/btr597},
	eprint = {
	          https://academic.oup.com/bioinformatics/article-pdf/28/1/112/50568519/bioinformatics
	          \_28\_1\_112.pdf},
}

@techreport{Crudu2022,
	title = {On the Role of the Zero Conditional Mean Assumption for Causal
	         Inference in Linear Models},
	author = {Crudu, Federico and Knaus, Michael and Mellace, Giovanni and Smits ,
	          Joeri},
	year = 2022,
	institution = {arXiv.org},
	type = {Papers},
	abstract = {Many econometrics textbooks imply that under mean independence of
	            the regressors and the error term, the OLS parameters have a causal
	            interpretation. We show that even when this assumption is satisfied
	            , OLS might identify a pseudo-parameter that does not have a causal
	            interpretation. Even assuming that the linear model is "structural"
	            creates some ambiguity in what the regression error represents and
	            whether the OLS estimand is causal. This issue applies equally to
	            linear IV and panel data models. To give these estimands a causal
	            interpretation, one needs to impose assumptions on a "causal" model
	            , e.g., using the potential outcome framework. This highlights that
	            causal inference requires causal, and not just stochastic,
	            assumptions.},
	url = {https://EconPapers.repec.org/RePEc:arx:papers:2211.09502},
}

@article{Yang2019,
	author = {Kun Yang and Justin Tu and Tian Chen},
	title = {Homoscedasticity: an overlooked critical assumption for linear
	         regression},
	volume = 32,
	number = 5,
	year = 2019,
	doi = {10.1136/gpsych-2019-100148},
	publisher = {General Psychiatry},
	abstract = {Linear regression is widely used in biomedical and psychosocial
	            research. A critical assumption that is often overlooked is
	            homoscedasticity. Unlike normality, the other assumption on data
	            distribution, homoscedasticity is often taken for granted when
	            fitting linear regression models. However, contrary to popular
	            belief, this assumption actually has a bigger impact on validity of
	            linear regression results than normality. In this report, we use
	            Monte Carlo simulation studies to investigate and compare their
	            effects on validity of inference.},
	URL = {https://gpsych.bmj.com/content/32/5/e100148},
	eprint = {https://gpsych.bmj.com/content/32/5/e100148.full.pdf},
	journal = {General Psychiatry},
}


@inbook{Baltagi2011,
	author = "Baltagi, Badi H.",
	title = "Violations of the Classical Assumptions",
	bookTitle = "Econometrics",
	year = 2011,
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "95--129",
	abstract = "In this chapter, we relax the assumptions made in Chapter 3 one by
	            one and study the effect of that on the OLS estimator. In case the
	            OLS estimator is no longer a viable estimator, we derive an
	            alternative estimator and propose some tests that will allow us to
	            check whether this assumption is violated.",
	isbn = "978-3-642-20059-5",
	doi = "10.1007/978-3-642-20059-5_5",
	url = "https://doi.org/10.1007/978-3-642-20059-5_5",
}

@article{schmidt2018,
	title = {Linear regression and the normality assumption},
	author = {Schmidt, Amand F and Finan, Chris},
	journal = {Journal of Clinical Epidemiology},
	volume = {98},
	pages = {146--151},
	year = 2018,
	publisher = {Elsevier},
}

@article{shrestha2020,
	title = {Detecting Multicollinearity in Regression Analysis},
	author = {Shrestha, Noora},
	journal = {American Journal of Applied Mathematics and Statistics},
	volume = 8,
	number = 2,
	pages = {39--42},
	year = 2020,
	doi = {10.12691/ajams-8-2-1},
}

@inbook{cutler2011,
	author = {Cutler, Adele and Cutler, David and Stevens, John},
	year = 2011,
	month = 1,
	pages = 157-176,
	title = {Random Forests},
	volume = 45,
	isbn = {978-1-4419-9325-0},
	journal = {Machine Learning - ML},
	doi = {10.1007/978-1-4419-9326-7_5},
}

@misc{zou2021,
	title = {Understanding the Generalization of Adam in Learning Neural Networks
	         with Proper Regularization},
	author = {Difan Zou and Yuan Cao and Yuanzhi Li and Quanquan Gu},
	year = {2021},
	eprint = {2108.11371},
	archivePrefix = {arXiv},
	primaryClass = {cs.LG},
}

@misc{Steorts15,
	author = {Steorts, Rebecca C.},
	title = {Bagging and Random Forests},
	year = 2015,
	url = {https://www2.stat.duke.edu/~
	       rcs46/lectures_2015/random-forest/randomforests.pdf},
}

@misc{jin2022,
	title = {Hyperparameter Importance for Machine Learning Algorithms},
	author = {Honghe Jin},
	year = {2022},
	eprint = {2201.05132},
	archivePrefix = {arXiv},
	primaryClass = {stat.ML},
}

@book{pytlak2009,
	title = {Conjugate Gradient Algorithms in Nonconvex Optimization},
	author = {Radosław Pytlak},
	publisher = {Springer},
	isbn = {9783540856337; 3540856331; 9783540856344; 354085634X},
	year = {2009},
	series = {Nonconvex Optimization and Its Applications №89},
	edition = {1},
	url = {libgen.li/file.php?md5=0488762b1858b9f00c7d894c34c59938},
}

@book{sra2011,
	title = {Optimization for Machine Learning (Neural Information Processing
	         series) },
	author = {Suvrit Sra, Sebastian Nowozin, Stephen J. Wright},
	publisher = {The MIT Press},
	isbn = {026201646X; 9780262016469},
	year = {2011},
	series = {Neural Information Processing series},
	url = {libgen.li/file.php?md5=7d0be65e811df10dd025655b433091ee},
}
@article{pek2018,
	AUTHOR = {Pek, Jolynn and Wong, Octavia and Wong, Augustine C. M.},
	TITLE = {How to Address Non-normality: A Taxonomy of Approaches, Reviewed, and
	         Illustrated},
	JOURNAL = {Frontiers in Psychology},
	VOLUME = {9},
	YEAR = {2018},
	URL = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.02104},
	DOI = {10.3389/fpsyg.2018.02104},
	ISSN = {1664-1078},
	ABSTRACT = {The linear model often serves as a starting point for applying
	            statistics in psychology. Often, formal training beyond the linear
	            model is limited, creating a potential pedagogical gap because of
	            the pervasiveness of data non-normality. We reviewed 61 recently
	            published undergraduate and graduate textbooks on introductory
	            statistics and the linear model, focusing on their treatment of
	            non-normality. This review identified at least eight distinct
	            methods suggested to address non-normality, which we organize into
	            a new taxonomy according to whether the approach: (a) remains
	            within the linear model, (b) changes the data, and (c) treats
	            normality as informative or as a nuisance. Because textbook
	            coverage of these methods was often cursory, and methodological
	            papers introducing these approaches are usually inaccessible to
	            non-statisticians, this review is designed to be the happy medium.
	            We provide a relatively non-technical review of advanced methods
	            which can address non-normality (and heteroscedasticity), thereby
	            serving a starting point to promote best practice in the
	            application of the linear model. We also present three empirical
	            examples to highlight distinctions between these methods'
	            motivations and results. The paper also reviews the current state
	            of methodological research in addressing non-normality within the
	            linear modeling framework. It is anticipated that our taxonomy will
	            provide a useful overview and starting place for researchers
	            interested in extending their knowledge in approaches developed to
	            address non-normality from the perspective of the linear model.},
}

@article{walsch2021,
	title = {Exploring the effects of omitted variable bias in physics education
	         research},
	author = {Walsh, Cole and Stein, Martin M. and Tapping, Ryan and Smith, Emily
	          M. and Holmes, N. G.},
	journal = {Phys. Rev. Phys. Educ. Res.},
	volume = {17},
	issue = {1},
	pages = {010119},
	numpages = {8},
	year = {2021},
	month = {Mar},
	publisher = {American Physical Society},
	doi = {10.1103/PhysRevPhysEducRes.17.010119},
	url = {https://link.aps.org/doi/10.1103/PhysRevPhysEducRes.17.010119},
}

