@inproceedings{neloy2019,
	title = {Ensemble Learning Based Rental Apartment Price Prediction Model by
	         Categorical Features Factoring},
	author = {Neloy, Asif Ahmed and Haque, H. M. Sadman and Ul Islam, Md. Mahmud },
	year = 2019,
	booktitle = {Proceedings of the 2019 11th International Conference on Machine
	             Learning and Computing - ICMLC '19},
	publisher = {ACM Press},
	address = {Zhuhai, China},
	pages = {350--356},
	doi = {10.1145/3318299.3318377},
}

@article{Aoki2023,
	author = {Aoki, Yoshitaka and Suzuki, Yuji and Nakajima, Yoshiki},
	title = {Critical considerations, including overfitting in regression models
	         and confounding in study designs for delirium follow-up},
	journal = {Journal of Anesthesia},
	year = 2023,
	month = 4,
	day = 1,
	volume = 37,
	number = 2,
	pages = {321-322},
	issn = {1438-8359},
	doi = {10.1007/s00540-022-03157-1},
	url = {https://doi.org/10.1007/s00540-022-03157-1},
}

@article{Stekhoven2011,
	author = {Stekhoven, Daniel J. and Bühlmann, Peter},
	title = "{MissForest—non-parametric missing value imputation for mixed-type
	         data}",
	journal = {Bioinformatics},
	volume = 28,
	number = 1,
	pages = {112--118},
	year = 2011,
	month = 10,
	abstract = { Modern data acquisition based on high-throughput technology is
	            often facing the problem of missing data. Algorithms commonly used
	            in the analysis of such large-scale data often depend on a complete
	            set. Missing value imputation offers a solution to this problem.
	            However, the majority of available imputation methods are
	            restricted to one type of variable only: continuous or categorical.
	            For mixed-type data, the different types are usually handled
	            separately. Therefore, these methods ignore possible relations
	            between variable types. We propose a non-parametric method which
	            can cope with different types of variables simultaneously.Results:
	            We compare several state of the art methods for the imputation of
	            missing values. We propose and evaluate an iterative imputation
	            method (missForest) based on a random forest. By averaging over
	            many unpruned classification or regression trees, random forest
	            intrinsically constitutes a multiple imputation scheme. Using the
	            built-in out-of-bag error estimates of random forest, we are able
	            to estimate the imputation error without the need of a test set.
	            Evaluation is performed on multiple datasets coming from a diverse
	            selection of biological fields with artificially introduced missing
	            values ranging from 10 to 30 percent. We show that missForest can
	            successfully handle missing values, particularly in datasets
	            including different types of variables. In our comparative study,
	            missForest outperforms other methods of imputation especially in
	            data settings where complex interactions and non-linear relations
	            are suspected. The out-of-bag imputation error estimates of
	            missForest prove to be adequate in all settings. Additionally,
	            missForest exhibits attractive computational efficiency and can
	            cope with high-dimensional data. },
	issn = {1367-4803},
	doi = {10.1093/bioinformatics/btr597},
	url = {https://doi.org/10.1093/bioinformatics/btr597},
	eprint = {
	          https://academic.oup.com/bioinformatics/article-pdf/28/1/112/50568519/bioinformatics
	          \_28\_1\_112.pdf},
}

@techreport{Crudu2022,
	title = {On the Role of the Zero Conditional Mean Assumption for Causal
	         Inference in Linear Models},
	author = {Crudu, Federico and Knaus, Michael and Mellace, Giovanni and Smits ,
	          Joeri},
	year = 2022,
	institution = {arXiv.org},
	type = {Papers},
	abstract = {Many econometrics textbooks imply that under mean independence of
	            the regressors and the error term, the OLS parameters have a causal
	            interpretation. We show that even when this assumption is satisfied
	            , OLS might identify a pseudo-parameter that does not have a causal
	            interpretation. Even assuming that the linear model is "structural"
	            creates some ambiguity in what the regression error represents and
	            whether the OLS estimand is causal. This issue applies equally to
	            linear IV and panel data models. To give these estimands a causal
	            interpretation, one needs to impose assumptions on a "causal" model
	            , e.g., using the potential outcome framework. This highlights that
	            causal inference requires causal, and not just stochastic,
	            assumptions.},
	url = {https://EconPapers.repec.org/RePEc:arx:papers:2211.09502},
}

@article{Yang2019,
	author = {Kun Yang and Justin Tu and Tian Chen},
	title = {Homoscedasticity: an overlooked critical assumption for linear
	         regression},
	volume = 32,
	number = 5,
	year = 2019,
	doi = {10.1136/gpsych-2019-100148},
	publisher = {General Psychiatry},
	abstract = {Linear regression is widely used in biomedical and psychosocial
	            research. A critical assumption that is often overlooked is
	            homoscedasticity. Unlike normality, the other assumption on data
	            distribution, homoscedasticity is often taken for granted when
	            fitting linear regression models. However, contrary to popular
	            belief, this assumption actually has a bigger impact on validity of
	            linear regression results than normality. In this report, we use
	            Monte Carlo simulation studies to investigate and compare their
	            effects on validity of inference.},
	URL = {https://gpsych.bmj.com/content/32/5/e100148},
	eprint = {https://gpsych.bmj.com/content/32/5/e100148.full.pdf},
	journal = {General Psychiatry},
}


@inbook{Baltagi2011,
	author = "Baltagi, Badi H.",
	title = "Violations of the Classical Assumptions",
	bookTitle = "Econometrics",
	year = 2011,
	publisher = "Springer Berlin Heidelberg",
	address = "Berlin, Heidelberg",
	pages = "95--129",
	abstract = "In this chapter, we relax the assumptions made in Chapter 3 one by
	            one and study the effect of that on the OLS estimator. In case the
	            OLS estimator is no longer a viable estimator, we derive an
	            alternative estimator and propose some tests that will allow us to
	            check whether this assumption is violated.",
	isbn = "978-3-642-20059-5",
	doi = "10.1007/978-3-642-20059-5_5",
	url = "https://doi.org/10.1007/978-3-642-20059-5_5",
}

@article{schmidt2018,
	title = {Linear regression and the normality assumption},
	author = {Schmidt, Amand F and Finan, Chris},
	journal = {Journal of Clinical Epidemiology},
	volume = 98,
	pages = {146--151},
	year = 2018,
	publisher = {Elsevier},
}

@article{shrestha2020,
	title = {Detecting Multicollinearity in Regression Analysis},
	author = {Shrestha, Noora},
	journal = {American Journal of Applied Mathematics and Statistics},
	volume = 8,
	number = 2,
	pages = {39--42},
	year = 2020,
	doi = {10.12691/ajams-8-2-1},
}

@inbook{cutler2011,
	author = {Cutler, Adele and Cutler, David and Stevens, John},
	year = 2011,
	month = 1,
	pages = "157-176",
	title = {Random Forests},
	volume = 45,
	isbn = {978-1-4419-9325-0},
	journal = {Machine Learning - ML},
	doi = {10.1007/978-1-4419-9326-7_5},
}

@misc{zou2021,
	title = {Understanding the Generalization of Adam in Learning Neural Networks
	         with Proper Regularization},
	author = {Difan Zou and Yuan Cao and Yuanzhi Li and Quanquan Gu},
	year = 2021,
	eprint = {2108.11371},
	archivePrefix = {arXiv},
	primaryClass = {cs.LG},
}


@misc{webscraper,
	title = {Web Scraper},
	howpublished = {\url{https://www.webscraper.io/}},
	note = {Accessed: 2023-11-25},
}

@book{dalpiaz2021,
	title = {Basics of Statistical Learning},
	author = {Dalpiaz, David},
	year = 2021,
	publisher = {University of Illinois},
	note = {Available online},
}

@article{hawinkel2023,
	title = {Out-of-Sample R2: Estimation and Inference},
	ISSN = {1537-2731},
	url = {http://dx.doi.org/10.1080/00031305.2023.2216252},
	DOI = {10.1080/00031305.2023.2216252},
	journal = {The American Statistician},
	publisher = {Informa UK Limited},
	author = {Hawinkel, Stijn and Waegeman, Willem and Maere, Steven},
	year = 2023,
	month = 6,
	pages = {1–11},
}

@misc{WalkScore2024,
	title = {Walk Score Methodology},
	author = {Walk Score},
	year = 2024,
	url = {https://www.walkscore.com/methodology.shtml},
}

@misc{Steorts15,
	author = {Steorts, Rebecca C.},
	title = {Bagging and Random Forests},
	year = 2015,
	url = {https://www2.stat.duke.edu/~
	       rcs46/lectures_2015/random-forest/randomforests.pdf},
}

@misc{jin2022,
	title = {Hyperparameter Importance for Machine Learning Algorithms},
	author = {Honghe Jin},
	year = 2022,
	eprint = {2201.05132},
	archivePrefix = {arXiv},
	primaryClass = {stat.ML},
}

@book{pytlak2009,
	title = {Conjugate Gradient Algorithms in Nonconvex Optimization},
	author = {Radosław Pytlak},
	publisher = {Springer},
	isbn = {9783540856337},
	year = 2009,
	series = {Nonconvex Optimization and Its Applications №89},
	edition = 1,
	url = {libgen.li/file.php?md5=0488762b1858b9f00c7d894c34c59938},
}

@book{sra2011,
	title = {Optimization for Machine Learning (Neural Information Processing
	         series) },
	author = {Suvrit Sra, Sebastian Nowozin, Stephen J. Wright},
	publisher = {The MIT Press},
	isbn = {9780262016469},
	year = 2011,
	series = {Neural Information Processing series},
	url = {libgen.li/file.php?md5=7d0be65e811df10dd025655b433091ee},
}

@misc{NeighbourhoodScoutCrime,
	title = {Crime Rates, Statistics and Crime Data for every Address in America},
	author = {Neighborhood Scout},
	year = 2023,
	howpublished = {\url{https://www.neighborhoodscout.com/crime}},
	note = {Accessed: 2023-12-1},
}

@article{shi2023,
	author = {Shi, Xintong and Cao, Wenzhi and Raschka, Sebastian},
	title = {Deep neural networks for rank-consistent ordinal regression based on
	         conditional probabilities},
	journal = {Pattern Analysis and Applications},
	year = {2023},
	month = {Aug},
	day = {01},
	volume = {26},
	number = {3},
	pages = {941-955},
	abstract = {In recent times, deep neural networks achieved outstanding
	            predictive performance on various classification and pattern
	            recognition tasks. However, many real-world prediction problems
	            have ordinal response variables, and this ordering information is
	            ignored by conventional classification losses such as the
	            multi-category cross-entropy. Ordinal regression methods for deep
	            neural networks address this. One such method is the CORAL method,
	            which is based on an earlier binary label extension framework and
	            achieves rank consistency among its output layer tasks by imposing
	            a weight-sharing constraint. However, while earlier experiments
	            showed that CORAL's rank consistency is beneficial for performance,
	            it is limited by a weight-sharing constraint in a neural network's
	            fully connected output layer, which may restrict the expressiveness
	            and capacity of a network trained using CORAL. We propose a new
	            method for rank-consistent ordinal regression without this
	            limitation. Our rank-consistent ordinal regression framework (CORN)
	            achieves rank consistency by a novel training scheme. This training
	            scheme uses conditional training sets to obtain the unconditional
	            rank probabilities through applying the chain rule for conditional
	            probability distributions. Experiments on various datasets
	            demonstrate the efficacy of the proposed method to utilize the
	            ordinal target information, and the absence of the weight-sharing
	            restriction improves the performance substantially compared to the
	            CORAL reference approach. Additionally, the suggested CORN method
	            is not tied to any specific architecture and can be utilized with
	            any deep neural network classifier to train it for ordinal
	            regression tasks.},
	issn = {1433-755X},
	doi = {10.1007/s10044-023-01181-9},
	url = {https://doi.org/10.1007/s10044-023-01181-9},
}

@article{lindner2022,
	author = {Lindner, Thomas and Puck, Jonas and Verbeke, Alain},
	title = {Beyond addressing multicollinearity: Robust quantitative analysis and
	         machine learning in international business research},
	journal = {Journal of International Business Studies},
	year = 2022,
	month = 9,
	day = 1,
	volume = 53,
	number = 7,
	pages = {1307-1314},
	abstract = {We reconcile the recommendations made by Kalnins (J Int Bus Stud,
	            2022) on the one hand and by Lindner, Puck and Verbeke (J Int Bus
	            Stud 51(3):283--298, 2020) on the other, on how international
	            business (IB) quantitative researchers should treat
	            multicollinearity. We explain that, in principle, treatment depends
	            on the underlying data generation process, but note that datasets
	            based on any single generation process are rare. In doing so, we
	            broaden the discussion to include how research methods should be
	            selected and robust statistical models built. In addition, we
	            highlight the importance of a comprehensive literature review in
	            selecting appropriate control variables. We also make suggestions
	            on addressing cross-level dependencies and selecting robustness
	            checks to avoid bias in statistical results. Finally, we go beyond
	            regression and include a broader palette of research methodologies
	            building on machine-learning approaches.},
	issn = {1478-6990},
	doi = {10.1057/s41267-022-00549-z},
	url = {https://doi.org/10.1057/s41267-022-00549-z},
}

@article{garlits2023,
	title = {Statistical Approaches for Establishing Appropriate Immunogenicity
	         Assay Cut Points: Impact of Sample Distribution, Sample Size, and
	         Outlier Removal},
	author = {Garlits, John and McAfee, Sean and Taylor, Jessica-Ann and Shum,
	          Enoch and Yang, Qi and Nunez, Emily and Kameron, Kristina and Fenech,
	          Keilah and Rodriguez, Jacqueline and Torri, Albert and Chen, Jihua
	          and Sumner, Giane and Partridge, Michael A.},
	year = {2023},
	journal = {The AAPS Journal},
	volume = {25},
	number = {2},
	pages = {1-14},
	doi = {10.1208/s12248-023-00806-5},
	url = {https://doi.org/10.1208/s12248-023-00806-5},
}
@misc{PocketSense,
	title = {The Landlord's Responsibilities When Multiple Renters Pay a Security
	         Deposit},
	author = {PocketSense},
	year = 2023,
	howpublished = {\url{
	                https://pocketsense.com/landlords-responsibilities-multiple-renters-pay-security-deposit-12105285.html
	                }},
	note = {Accessed: 2023-12-1},
}

@article{walsch2021,
	title = {Exploring the effects of omitted variable bias in physics education
	         research},
	author = {Walsh, Cole and Stein, Martin M. and Tapping, Ryan and Smith, Emily
	          M. and Holmes, N. G.},
	journal = {Phys. Rev. Phys. Educ. Res.},
	volume = 17,
	issue = 1,
	pages = {010119},
	numpages = 8,
	year = 2021,
	month = 3,
	publisher = {American Physical Society},
	doi = {10.1103/PhysRevPhysEducRes.17.010119},
	url = {https://link.aps.org/doi/10.1103/PhysRevPhysEducRes.17.010119},
}

@misc{kingma2017,
	title = {Adam: A Method for Stochastic Optimization},
	author = {Diederik P. Kingma and Jimmy Ba},
	year = 2017,
	eprint = {1412.6980},
	archivePrefix = {arXiv},
	primaryClass = {cs.LG},
}

@article{marcot2021,
	author = {Marcot, Bruce G. and Hanea, Anca M.},
	title = {What is an optimal value of k in k-fold cross-validation in discrete
	         Bayesian network analysis?},
	journal = {Computational Statistics},
	year = 2021,
	month = 9,
	day = 1,
	volume = 36,
	number = 3,
	pages = {2009--2031},
	abstract = {Cross-validation using randomized subsets of data---known as
	            k-fold cross-validation---is a powerful means of testing the
	            success rate of models used for classification. However, few if any
	            studies have explored how values of k (number of subsets) affect
	            validation results in models tested with data of known statistical
	            properties. Here, we explore conditions of sample size, model
	            structure, and variable dependence affecting validation outcomes in
	            discrete Bayesian networks (BNs). We created 6 variants of a BN
	            model with known properties of variance and collinearity, along
	            with data sets of n{\thinspace}={\thinspace}50, 500, and 5000
	            samples, and then tested classification success and evaluated CPU
	            computation time with seven levels of folds (k{\thinspace}={
	            \thinspace}2, 5, 10, 20, n − 5, n − 2, and n − 1). Classification
	            error declined with increasing n, particularly in BN models with
	            high multivariate dependence, and declined with increasing k,
	            generally levelling out at k{\thinspace}={\thinspace}10, although k
	            {\thinspace}={\thinspace}5 sufficed with large samples (n{
	            \thinspace}={\thinspace}5000). Our work supports the common use of
	            k{\thinspace}={\thinspace}10 in the literature, although in some
	            cases k{\thinspace}={\thinspace}5 would suffice with BN models
	            having independent variable structures.},
	issn = {1613-9658},
	doi = {10.1007/s00180-020-00999-9},
	url = {https://doi.org/10.1007/s00180-020-00999-9},
}

@article{rosen1974,
	author = {Rosen, Sherwin},
	title = {Hedonic Prices and Implicit Markets: Product Differentiation in Pure
	         Competition},
	journal = {Journal of Political Economy},
	volume = 82,
	number = 1,
	pages = {34--55},
	year = 1974,
	doi = {10.1086/260169},
	url = {https://doi.org/10.1086/260169},
}


@inbook{taylor2003,
	author = "Taylor, Laura O.",
	editor = "Champ, Patricia A. and Boyle, Kevin J. and Brown, Thomas C.",
	title = "The Hedonic Method",
	bookTitle = "A Primer on Nonmarket Valuation",
	year = 2003,
	publisher = "Springer Netherlands",
	address = "Dordrecht",
	pages = "331--393",
	abstract = "Heterogeneous or differentiated goods are products whose
	            characteristics vary in such a way that there are distinct product
	            varieties even though the commodity is sold in one market (e.g.,
	            cars, computers, houses). The variation in product variety gives
	            rise to variations in product prices within each market. The
	            hedonic method for non-market valuation relies on market
	            transactions for these differentiated goods to determine the value
	            of key underlying characteristics. For instance, by observing the
	            price differential between two product varieties that vary only by
	            one characteristic (e.g., two identical cars, but with one having
	            more horsepower than the other), we indirectly observe the monetary
	            trade-offs individuals are willing to make with respect to the
	            changes in this characteristic. As such, the hedonic method is an
	            ``indirect'' valuation method in which we do not observe the value
	            consumers have for the characteristic directly, but infer it from
	            observable market transactions.",
	isbn = "978-94-007-0826-6",
	doi = "10.1007/978-94-007-0826-6_10",
	url = "https://doi.org/10.1007/978-94-007-0826-6_10",
}

@inproceedings{joshi2022,
	author = "Joshi, Ishan and Mudgil, Pooja and Bisht, Arpit",
	editor = "Gupta, Deepak and Khanna, Ashish and Hassanien, Aboul Ella and Anand
	          , Sameer and Jaiswal, Ajay",
	title = "House Price Forecasting by Implementing Machine Learning Algorithms:
	         A Comparative Study",
	booktitle = "International Conference on Innovative Computing and
	             Communications",
	year = 2022,
	publisher = "Springer Nature Singapore",
	address = "Singapore",
	pages = "63--71",
	abstract = "Discerning property value via state-of-the-art machine learning
	            techniques can evolve the current real-estate market and expose it
	            to the technological frontiers of the modern world. This can
	            potentially have sanguine domino effects such as opening the market
	            to new investors as a result of technically backed price values.
	            The current research paper strives to capitalize on this
	            opportunity by analyzing information and data from an existing
	            online marketplace for buyers and sellers in this industry. It is
	            conjectured that precise prediction of house prices in a particular
	            location through data analytics will create a candid market where
	            prices are not arbitrary, ensuring openness in the market through
	            P2P opportunities which will eliminate middleman charges. The
	            research ventures to extrapolate machine learning techniques to
	            create a model that predicts house prices in Bangalore using a
	            plethora of algorithms such as linear regression, bagging
	            classifier, K-nearest neighbour, XGB, decision tree, gradient
	            boosting, and random forest. An incremental approach is deployed to
	            gather and streamline data, clean, visualize, model and evaluate
	            the models produced. The research is completed with a result from
	            the comparative study, showing the most appropriate algorithm for
	            the given data available is the random forest algorithm.",
	isbn = "978-981-19-3679-1",
}

@article{zietz2008,
	author = {Zietz, Joachim and Zietz, Emily Norman and Sirmans, G. Stacy},
	title = {Determinants of House Prices: A Quantile Regression Approach},
	journal = {The Journal of Real Estate Finance and Economics},
	year = 2008,
	month = 11,
	day = 1,
	volume = 37,
	number = 4,
	pages = "317-333",
	abstract = {OLS regression has typically been used in housing research to
	            determine the relationship of a particular housing characteristic
	            with selling price. Results differ across studies, not only in
	            terms of size of OLS coefficients and statistical significance, but
	            sometimes in direction of effect. This study suggests that some of
	            the observed variation in the estimated prices of housing
	            characteristics may reflect the fact that characteristics are not
	            priced the same across a given distribution of house prices. To
	            examine this issue, this study uses quantile regression, with and
	            without accounting for spatial autocorrecation, to identify the
	            coefficients of a large set of diverse variables across different
	            quantiles. The results show that purchasers of higher-priced homes
	            value certain housing characteristics such as square footage and
	            the number of bathrooms differently from buyers of lower-priced
	            homes. Other variables such as age are also shown to vary across
	            the distribution of house prices.},
	issn = {1573-045X},
	doi = {10.1007/s11146-007-9053-7},
	url = {https://doi.org/10.1007/s11146-007-9053-7},
}

@article{yoshida2022,
	author = {Yoshida, Takahiro and Murakami, Daisuke and Seya, Hajime},
	title = {Spatial Prediction of Apartment Rent using Regression-Based and
	         Machine Learning-Based Approaches with a Large Dataset},
	journal = {The Journal of Real Estate Finance and Economics},
	year = 2022,
	month = 11,
	day = 28,
	abstract = {Employing a large dataset (at most, the order of n{\thinspace}={
	            \thinspace}106), this study attempts enhance the literature on the
	            comparison between regression and machine learning-based rent price
	            prediction models by adding new empirical evidence and considering
	            the spatial dependence of the observations. The regression-based
	            approach incorporates the nearest neighbor Gaussian processes
	            (NNGP) model, enabling the application of kriging to large
	            datasets. In contrast, the machine learning-based approach utilizes
	            typical models: extreme gradient boosting (XGBoost), random forest
	            (RF), and deep neural network (DNN). The out-of-sample prediction
	            accuracy of these models was compared using Japanese apartment rent
	            data, with a varying order of sample sizes (i.e., n{\thinspace}={
	            \thinspace}104, 105, 106). The results showed that, as the sample
	            size increased, XGBoost and RF outperformed NNGP with higher
	            out-of-sample prediction accuracy. XGBoost achieved the highest
	            prediction accuracy for all sample sizes and error measures in both
	            logarithmic and real scales and for all price bands if the
	            distribution of rents is similar in training and test data. A
	            comparison of several methods to account for the spatial dependence
	            in RF showed that simply adding spatial coordinates to the
	            explanatory variables may be sufficient.},
	issn = {1573-045X},
	doi = {10.1007/s11146-022-09929-6},
	url = {https://doi.org/10.1007/s11146-022-09929-6},
}
